{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "_UkmnhYjFXsx"
   },
   "outputs": [],
   "source": [
    "#!git config --global user.email \"jonnygaese@gmail.com\"\n",
    "#!git config --global user.name \"JohnnyG502\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoOotFoMT0S9"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y0BCN_hlBzuh",
    "outputId": "72339c6a-9c02-4d24-96a0-e6593c28eaf9"
   },
   "outputs": [],
   "source": [
    "#!pip install colorama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hg0Y93n1Faxv",
    "outputId": "438f7139-12b5-4316-e02b-2f0d7730192b"
   },
   "outputs": [],
   "source": [
    "#!git clone https://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@github.com/JohnnyG502/pommermanAI.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xy1CHIukFfgY",
    "outputId": "26d6979b-6ca0-4c83-9132-b87a13c43844"
   },
   "outputs": [],
   "source": [
    "#%cd pommermanAI/reinforcement_learning/johannes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FnuvZ2IpFhvj",
    "outputId": "65c0cad5-28d8-4fbb-bfb2-ce81eb0d51d8"
   },
   "outputs": [],
   "source": [
    "#git pull origin main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "14bTPofhHzgr"
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.insert(0,'/content/reinforcement_learning/johannes/model_action_prun_colab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VfVhuaeHBmh9",
    "outputId": "3b5c4596-40ca-411d-aa4d-09b9d8af23e5"
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/MultiAgentLearning/playground.git\n",
    "#%cd playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "C7_Q-6zhDJV2",
    "outputId": "708112e5-0ef5-4453-89a5-c3672b38f0cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n"
     ]
    }
   ],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OeIGU3hgFOrL",
    "outputId": "2339416d-11f5-4b64-df5d-3c76ea5ef614"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pommerman'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\JOHANN~1\\AppData\\Local\\Temp/ipykernel_14548/2789005471.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcolorama\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpommerman\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0magents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pommerman'"
     ]
    }
   ],
   "source": [
    "import colorama\n",
    "from pommerman import agents\n",
    "from collections import Counter\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import numpy.matlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSJfyfjoMpnT"
   },
   "source": [
    "# Neuer Abschnitt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Xlx6gIIBJEz"
   },
   "outputs": [],
   "source": [
    "import pommerman\n",
    "from pommerman import agents\n",
    "import sys\n",
    "import gym\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from collections import Counter\n",
    "import multiprocessing as mp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "class Leif(agents.BaseAgent):\n",
    "    def __init__(self, model):\n",
    "        super(Leif, self).__init__()\n",
    "        self.model     = model\n",
    "        self.states    = []\n",
    "        self.actions   = []\n",
    "        self.hidden    = []\n",
    "        self.values    = []\n",
    "        self.probs     = []\n",
    "        self.debug     = False\n",
    "        self.stochastic = True\n",
    "        \n",
    "    def translate_obs(self, o):\n",
    "        obs_width = self.model.obs_width\n",
    "        \n",
    "        board = o['board'].copy()\n",
    "        agents = np.column_stack(np.where(board > 10))\n",
    "\n",
    "        for i, agent in enumerate(agents): \n",
    "            agent_id = board[agent[0], agent[1]]\n",
    "            if agent_id not in o['alive']: # < this fixes a bug >\n",
    "                board[agent[0], agent[1]] = 0\n",
    "            else:\n",
    "                board[agent[0], agent[1]] = 11\n",
    "\n",
    "        obs_radius = obs_width//2\n",
    "        pos = np.asarray(o['position'])\n",
    "\n",
    "        # board\n",
    "        board_pad = np.pad(board, (obs_radius,obs_radius), 'constant', constant_values=1)\n",
    "        self.board_cent = board_cent = board_pad[pos[0]:pos[0]+2*obs_radius+1,pos[1]:pos[1]+2*obs_radius+1]\n",
    "\n",
    "        # bomb blast strength\n",
    "        bbs = o['bomb_blast_strength']\n",
    "        bbs_pad = np.pad(bbs, (obs_radius,obs_radius), 'constant', constant_values=0)\n",
    "        self.bbs_cent = bbs_cent = bbs_pad[pos[0]:pos[0]+2*obs_radius+1,pos[1]:pos[1]+2*obs_radius+1]\n",
    "\n",
    "        # bomb life\n",
    "        bl = o['bomb_life']\n",
    "        bl_pad = np.pad(bl, (obs_radius,obs_radius), 'constant', constant_values=0)\n",
    "        self.bl_cent = bl_cent = bl_pad[pos[0]:pos[0]+2*obs_radius+1,pos[1]:pos[1]+2*obs_radius+1]\n",
    "\n",
    "        return np.concatenate((\n",
    "            board_cent, bbs_cent, bl_cent,\n",
    "            o['blast_strength'], o['can_kick'], o['ammo']), axis=None)\n",
    "\n",
    "    def act(self, obs, action_space):\n",
    "        obs = self.translate_obs(obs)\n",
    "        \n",
    "        last_hn, last_cn = self.hidden[-1][0], self.hidden[-1][1]\n",
    "        obs = torch.from_numpy(obs).float().to(self.model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            last_hn, last_cn = torch.tensor(last_hn).unsqueeze(0), torch.tensor(last_cn).unsqueeze(0)\n",
    "            probs, val, hn, cn = self.model(obs.unsqueeze(0), last_hn, last_cn, self.debug)\n",
    "            \n",
    "            if self.debug: \n",
    "                print(\"hn mean:\", hn.mean(), \"hn std:\", hn.std(), \"cn mean:\", cn.mean(), \"cn std:\", cn.std())\n",
    "            \n",
    "            probs_softmaxed = F.softmax(probs, dim=-1)\n",
    "\n",
    "            if self.stochastic:\n",
    "                action = Categorical(probs_softmaxed).sample().item()\n",
    "            else: \n",
    "                action = probs_softmaxed.max(1, keepdim=True)[1].item()\n",
    "\n",
    "        self.actions.append(action)\n",
    "        self.states.append(obs.squeeze(0).numpy())\n",
    "        self.probs.append(probs.detach().numpy())\n",
    "        self.values.append(val.detach().item())\n",
    "        self.hidden.append(\n",
    "            ( hn.squeeze(0).clone().detach().numpy(), \n",
    "              cn.squeeze(0).clone().detach().numpy() ))\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def clear(self):\n",
    "        del self.states[:]\n",
    "        del self.actions[:]\n",
    "        del self.hidden[:]\n",
    "        del self.probs[:]\n",
    "        del self.values[:]\n",
    "\n",
    "        self.hidden.insert(0, self.model.init_rnn())\n",
    "\n",
    "        return self.states, self.actions, self.hidden, self.probs, self.values\n",
    "\n",
    "class Stoner(agents.BaseAgent):\n",
    "        def __init__(self): super(Stoner, self).__init__()\n",
    "        def act(self, obs, action_space): \n",
    "            return 0#random.randint(1,4) #0\n",
    "\n",
    "class A2CNet(nn.Module):\n",
    "    def __init__(self, gpu = True): \n",
    "        super(A2CNet, self).__init__()\n",
    "        \n",
    "        self.gamma             = 0.50   # Discount factor for rewards (default 0.99)\n",
    "        self.entropy_coef      = 0.01   # Entropy coefficient (0.01)\n",
    "        self.obs_width = w     = 17     # Window width/height (must be uneven)\n",
    "        self.lr                = 0.001  # 3e-2\n",
    "\n",
    "        self.inputs_to_conv = ic  = 3*(w**2)           # 3 boards\n",
    "        self.inputs_to_fc   = ifc = 3                  # blast strength, can_kick, ammo\n",
    "        self.conv_channels  = cc  = 45                 # number of conv outputs\n",
    "        #self.flat_after_c  = fac = cc * (w-3) * (w-3) # cc * (w-4) * (w-4) # flattened num after conv\n",
    "        #self.flat_after_c  = fac = cc * (w-2) * (w-2) # cc * (w-4) * (w-4) # flattened num after conv\n",
    "        #self.flat_after_c  = fac = cc * (w-cc) * (w-cc) # cc * (w-4) * (w-4) # flattened num after conv\n",
    "        self.flat_after_c  = fac = 13005\n",
    "\n",
    "        self.fc1s, self.fc2s, self.fc3s = 1024, 512, 64\n",
    "        \n",
    "        self.rnn_input_size   = self.fc2s \n",
    "        self.rnn_hidden_size  = 64\n",
    "        \n",
    "        self.conv1 = torch.nn.Conv2d(3,  cc,   kernel_size=3, stride=1, padding=1, groups=3)\n",
    "        self.conv2 = torch.nn.Conv2d(cc, cc,   kernel_size=3, stride=1, padding=1, groups=3) #dilation=2, stride=1, padding=1)\n",
    "        self.conv3 = torch.nn.Conv2d(cc, cc,   kernel_size=3, stride=1, padding=1, groups=3)\n",
    "        self.conv4 = torch.nn.Conv2d(cc, cc,   kernel_size=3, stride=1, padding=1, groups=3)\n",
    "\n",
    "        self.bn1, self.bn2 = nn.BatchNorm2d(cc), nn.BatchNorm2d(cc)\n",
    "        self.bn3, self.bn4 = nn.BatchNorm2d(cc), nn.BatchNorm2d(cc)\n",
    "\n",
    "        self.fc_after_conv1 = nn.Linear(fac, self.fc1s)\n",
    "        self.fc_after_conv2 = nn.Linear(self.fc1s + ifc, self.fc2s)\n",
    "        self.fc_after_conv3 = nn.Linear(self.fc2s, self.fc2s)\n",
    "        self.fc_after_conv4 = nn.Linear(self.fc2s, self.fc2s)\n",
    "        \n",
    "        self.rnn = torch.nn.LSTMCell(self.rnn_input_size, self.rnn_hidden_size)\n",
    "\n",
    "        self.fc_after_rnn_1 = nn.Linear(self.rnn_hidden_size, self.fc3s)\n",
    "        # self.fc_after_rnn_2 = nn.Linear(self.fc3s, self.fc3s)\n",
    "        # self.fc_after_rnn_3 = nn.Linear(self.fc3s, self.fc3s)\n",
    "        # self.fc_after_rnn_4 = nn.Linear(self.fc3s, self.fc3s)\n",
    "        \n",
    "        self.action_head = nn.Linear(self.fc3s, 6)\n",
    "        self.value_head  = nn.Linear(self.fc2s, 1)\n",
    "\n",
    "        self.optimizer   = optim.Adam(self.parameters(), lr=self.lr)\n",
    "        self.eps         = np.finfo(np.float32).eps.item()\n",
    "        \n",
    "        self.device = torch.device(\"cuda:0\" if gpu and torch.cuda.is_available() else \"cpu\")\n",
    "        print(self.device)\n",
    "        if self.device.type == 'cuda': self.cuda()\n",
    "        return None\n",
    "\n",
    "    def forward(self, x, hn, cn, debug = False):\n",
    "        batch_size = x.shape[0]\n",
    "        w, wh = self.obs_width, self.obs_width**2\n",
    "        \n",
    "        boards  = x[:,      0:wh].view(batch_size, 1, w, w)\n",
    "        bbs     = x[:,   wh:wh*2].view(batch_size, 1, w, w)\n",
    "        bl      = x[:, wh*2:wh*3].view(batch_size, 1, w, w)\n",
    "        \n",
    "        rest    = x[:, wh*3:]\n",
    "        to_conv = torch.cat([boards, bbs, bl], 1)\n",
    "        \n",
    "        xc = self.conv1(to_conv)\n",
    "        xc = self.bn1(xc)\n",
    "        xc = F.relu(xc)\n",
    "\n",
    "        xc = self.conv2(xc)\n",
    "        xc = self.bn2(xc)\n",
    "        xc = F.relu(xc)\n",
    "\n",
    "        xc = self.conv3(xc)\n",
    "        xc = self.bn3(xc)\n",
    "        xc = F.relu(xc)\n",
    "\n",
    "        xc = self.conv4(xc)\n",
    "        xc = self.bn4(xc)\n",
    "        xc = F.relu(xc)\n",
    "        \n",
    "        xc = xc.view(batch_size, -1)\n",
    "        xc = self.fc_after_conv1(xc)\n",
    "        xc = F.relu(xc)\n",
    "        \n",
    "        xc = torch.cat((xc, rest), 1)\n",
    "        xc = self.fc_after_conv2(xc)\n",
    "        xc = F.relu(xc)\n",
    "\n",
    "        xc = self.fc_after_conv3(xc)\n",
    "        xc = F.relu(xc)\n",
    "        \n",
    "        xc = self.fc_after_conv4(xc)\n",
    "        xc = F.relu(xc)\n",
    "        \n",
    "        # if not debug:\n",
    "        #     print(xc[0, :].mean(), xc[0, :].std())\n",
    "\n",
    "        if debug == True:   \n",
    "            mm = xc[0, :].mean()\n",
    "            nn = xc[0, :].std()\n",
    "        \n",
    "        values  = self.value_head(xc)\n",
    "        hn, cn  = self.rnn(xc, (hn, cn))\n",
    "        xc = hn #torch.cat((xc, hn), 1)\n",
    "        \n",
    "        if debug == True:\n",
    "            mm1 = xc[0, :].mean()\n",
    "            nn1 = xc[0, :].std()\n",
    "            print(\"Before rnn:\", (mm,nn), \"After rnn:\", (mm1,nn1))\n",
    "        \n",
    "        xc = self.fc_after_rnn_1(xc)\n",
    "        xc = F.relu(xc)\n",
    "        \n",
    "        # xc = self.fc_after_rnn_2(xc)\n",
    "        # xc = F.relu(xc)\n",
    "\n",
    "        # xc = self.fc_after_rnn_3(xc)\n",
    "        # xc = F.relu(xc)\n",
    "        \n",
    "        # xc = self.fc_after_rnn_4(xc)\n",
    "        # xc = F.relu(xc)\n",
    "        \n",
    "        probs = self.action_head(xc)\n",
    "        \n",
    "        return probs, values, hn, cn\n",
    "        \n",
    "    def init_rnn(self):\n",
    "        device = self.device\n",
    "        s = self.rnn_hidden_size\n",
    "        return (torch.zeros(s).detach().numpy(), torch.zeros(s).detach().numpy())\n",
    "    \n",
    "    def discount_rewards(self, _rewards):\n",
    "        R = 0\n",
    "        gamma = self.gamma\n",
    "        rewards = []\n",
    "        for r in _rewards[::-1]:\n",
    "            R = r + gamma * R\n",
    "            rewards.insert(0, R)\n",
    "\n",
    "        # rewards = np.array(rewards) \n",
    "        # rewards = (rewards - rewards.mean()) / (rewards.std() + self.eps)\n",
    "        \n",
    "        return rewards #torch.from_numpy(rewards).to(self.device)\n",
    "\n",
    "def naked_env(agent_list):\n",
    "    env = gym.make('PommeRadioCompetition-v2')\n",
    "    env._num_items = 0\n",
    "    env._num_wood  = 0\n",
    "    env._num_rigid = 0\n",
    "    env._max_steps = 100\n",
    "\n",
    "    for id, agent in enumerate(agent_list):\n",
    "        assert isinstance(agent, agents.BaseAgent)\n",
    "        agent.init_agent(id, env.spec._kwargs['game_type'])\n",
    "\n",
    "    env.set_agents(agent_list)\n",
    "    env.set_init_game_state(None)\n",
    "    env.set_render_mode('human')\n",
    "    return env\n",
    "\n",
    "def normal_env(agent_list):\n",
    "    env = gym.make('PommeRadioCompetition-v2')\n",
    "    \n",
    "    for id, agent in enumerate(agent_list):\n",
    "        assert isinstance(agent, agents.BaseAgent)\n",
    "        agent.init_agent(id, env.spec._kwargs['game_type'])\n",
    "\n",
    "    env.set_agents(agent_list)\n",
    "    env.set_init_game_state(None)\n",
    "    env.set_render_mode('human')\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_blBAK9TFOrR",
    "outputId": "a1f68296-a962-4c39-ec95-780cc394f9d1"
   },
   "outputs": [],
   "source": [
    "\n",
    "ROLLOUTS_PER_BATCH = 1\n",
    "batch = []\n",
    "\n",
    "\n",
    "class World:\n",
    "    def __init__(self, init_gmodel=True):\n",
    "        if init_gmodel:\n",
    "            self.gmodel = A2CNet(gpu=True)  # Global model\n",
    "\n",
    "        self.model = A2CNet(gpu=False)  # Agent (local) model\n",
    "        self.leif = Leif(self.model)\n",
    "        self.stoner = Stoner()\n",
    "\n",
    "        self.agent_list = [\n",
    "            self.leif,\n",
    "            # self.stoner\n",
    "            agents.SimpleAgent(),\n",
    "            agents.SimpleAgent(),\n",
    "            agents.SimpleAgent()\n",
    "        ]\n",
    "        self.env = normal_env(self.agent_list)  # naked_env\n",
    "        fmt = {\n",
    "            'int': self.color_sign,\n",
    "            'float': self.color_sign\n",
    "        }\n",
    "        np.set_printoptions(formatter=fmt, linewidth=300)\n",
    "        pass\n",
    "\n",
    "    def color_sign(self, x):\n",
    "        if x == 0:\n",
    "            c = colorama.Fore.LIGHTBLACK_EX\n",
    "        elif x == 1:\n",
    "            c = colorama.Fore.BLACK\n",
    "        elif x == 2:\n",
    "            c = colorama.Fore.BLUE\n",
    "        elif x == 3:\n",
    "            c = colorama.Fore.RED\n",
    "        elif x == 4:\n",
    "            c = colorama.Fore.RED\n",
    "        elif x == 10:\n",
    "            c = colorama.Fore.YELLOW\n",
    "        else:\n",
    "            c = colorama.Fore.WHITE\n",
    "        x = '{0: <2}'.format(x)\n",
    "        return f'{c}{x}{colorama.Fore.RESET}'\n",
    "\n",
    "\n",
    "def do_rollout(env, leif, do_print=False):\n",
    "    done, state = False, env.reset()\n",
    "    rewards, dones = [], []\n",
    "    states, actions, hidden, probs, values = leif.clear()\n",
    "    old_state = None\n",
    "    last_action = 0\n",
    "\n",
    "    while not done and 10 in state[0]['alive']:\n",
    "        if do_print:\n",
    "            time.sleep(0.1)\n",
    "            os.system('clear')\n",
    "            print(state[0]['board'])\n",
    "\n",
    "        action = env.act(state)\n",
    "        state, start_rewards, done, info = env.step(action)\n",
    "        action = action[0]\n",
    "        if old_state is None:\n",
    "            old_state = state\n",
    "        reward = get_reward(state, old_state, 0, action, last_action)\n",
    "        # print(str(state[0]['position']) + str(old_state[0]['position']) + str(reward))\n",
    "        old_state = state\n",
    "        last_action = action\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "    hidden = hidden[:-1].copy()\n",
    "    hns, cns = [], []\n",
    "    for hns_cns_tuple in hidden:\n",
    "        hns.append(hns_cns_tuple[0])\n",
    "        cns.append(hns_cns_tuple[1])\n",
    "\n",
    "    rewards = rewards[:len(values)]\n",
    "\n",
    "    return (states.copy(),\n",
    "            actions.copy(),\n",
    "            rewards, dones,\n",
    "            (hns, cns),\n",
    "            probs.copy(),\n",
    "            values.copy())\n",
    "\n",
    "\n",
    "def get_reward(state, old_state, agent_nr, action, last_action):\n",
    "    # developer note: on the board:\n",
    "    # 0: nothing, 1: unbreakable wall, 2: wall, 3: bomb, 4: flames, 6,7,8: pick-ups:  11,12 and 13: enemies\n",
    "    reward = 0\n",
    "    # penalty for dying\n",
    "    if 10 not in state[0]['alive']:\n",
    "        reward -= 1\n",
    "\n",
    "    # reward stage 0:\n",
    "    # teach the agent to move and not make invalid actions (move into walls, place bombs when you have no ammo)\n",
    "    ammo = old_state[agent_nr]['ammo']\n",
    "    if action != 5:\n",
    "        if state[agent_nr]['position'] == old_state[agent_nr]['position']:\n",
    "            reward -= 0.03\n",
    "    elif ammo == 0:\n",
    "        reward -= 0.03\n",
    "\n",
    "    # reward stage 1: teach agent to bomb walls (and enemies)\n",
    "    # compute adjacent squares\n",
    "    position = state[agent_nr]['position']\n",
    "    adj = [(i, j) for i in (-1, 0, 1) for j in (-1, 0, 1) if not ((i == j) or i + j == 0)]\n",
    "    adjacent = numpy.matlib.repmat(position, 4, 1)\n",
    "    adjacent = adjacent - np.asarray(adj)\n",
    "    # limit adjacent squares to only include inside board\n",
    "    adjacent = np.clip(adjacent, 0, 10)\n",
    "    if action == 5 and ammo > 0:\n",
    "        board = state[agent_nr]['board']\n",
    "        for xy in adjacent:\n",
    "            square_val = board[xy[0]][xy[1]]\n",
    "            if square_val == 2:\n",
    "                reward += 0.2\n",
    "            elif square_val == 11 or square_val == 12 or square_val == 13:\n",
    "                reward += 0.5\n",
    "\n",
    "    # reward stage2: teach agent to not stand on or beside bombs\n",
    "    # reward /= 4\n",
    "    bomb_life = state[agent_nr]['bomb_life']\n",
    "    # if we stand on a bomb or next to bomb\n",
    "    just_placed_bomb = np.logical_xor(last_action == 5, action == 5)\n",
    "    if bomb_life[position] > 0 and not just_placed_bomb:\n",
    "        reward -= 0.1 * (9-bomb_life[position])\n",
    "    for xy in adjacent:\n",
    "        if bomb_life[xy[0]][xy[1]] > 0:\n",
    "            reward -= 0.05 * (9-bomb_life[xy[0]][xy[1]])\n",
    "\n",
    "    # reward agent for picking up power-ups\n",
    "    blast_strength = state[agent_nr]['blast_strength']\n",
    "    old_blast_strength = old_state[agent_nr]['blast_strength']\n",
    "    can_kick = int(state[agent_nr]['can_kick'])\n",
    "    old_can_kick = int(old_state[agent_nr]['can_kick'])\n",
    "    reward += (can_kick-old_can_kick)*0.02\n",
    "    # reward += (max_ammo-old_max_ammo)*0.02 #TODO, see arguments\n",
    "    reward += (blast_strength-old_blast_strength)*0.02\n",
    "    return reward\n",
    "\n",
    "\n",
    "def gmodel_train(gmodel, states, hns, cns, actions, rewards, gae):\n",
    "    states, hns, cns = torch.stack(states), torch.stack(hns, dim=0), torch.stack(cns, dim=0)\n",
    "    gmodel.train()\n",
    "    probs, values, _, _ = gmodel(states.to(gmodel.device), hns.to(gmodel.device), cns.to(gmodel.device), debug=False)\n",
    "\n",
    "    prob = F.softmax(probs, dim=-1)\n",
    "    log_prob = F.log_softmax(probs, dim=-1)\n",
    "    entropy = -(log_prob * prob).sum(1)\n",
    "\n",
    "    log_probs = log_prob[range(0, len(actions)), actions]\n",
    "    advantages = torch.tensor(rewards).to(gmodel.device) - values.squeeze(1)\n",
    "    value_loss = advantages.pow(2) * 0.5\n",
    "    policy_loss = -log_probs * torch.tensor(gae).to(gmodel.device) - gmodel.entropy_coef * entropy\n",
    "\n",
    "    gmodel.optimizer.zero_grad()\n",
    "    pl = policy_loss.sum()\n",
    "    vl = value_loss.sum()\n",
    "    loss = pl + vl\n",
    "    loss.backward()\n",
    "    gmodel.optimizer.step()\n",
    "\n",
    "    return loss.item(), pl.item(), vl.item()\n",
    "\n",
    "\n",
    "def unroll_rollouts(gmodel, list_of_full_rollouts):\n",
    "    gamma = gmodel.gamma\n",
    "    tau = 1\n",
    "\n",
    "    states, actions, rewards, hns, cns, gae = [], [], [], [], [], []\n",
    "    for (s, a, r, d, h, p, v) in list_of_full_rollouts:\n",
    "        states.extend(torch.tensor(s))\n",
    "        actions.extend(a)\n",
    "        rewards.extend(gmodel.discount_rewards(r))\n",
    "\n",
    "        hns.extend([torch.tensor(hh) for hh in h[0]])\n",
    "        cns.extend([torch.tensor(hh) for hh in h[1]])\n",
    "\n",
    "        # Calculate GAE\n",
    "        last_i, _gae, __gae = len(r) - 1, [], 0\n",
    "        for i in reversed(range(len(r))):\n",
    "            next_val = v[i + 1] if i != last_i else 0\n",
    "            delta_t = r[i] + gamma * next_val - v[i]\n",
    "            __gae = __gae * gamma * tau + delta_t\n",
    "            _gae.insert(0, __gae)\n",
    "\n",
    "        gae.extend(_gae)\n",
    "\n",
    "    return states, hns, cns, actions, rewards, gae\n",
    "\n",
    "\n",
    "def train(world):\n",
    "    model, gmodel = world.model, world.gmodel\n",
    "    leif, env = world.leif, world.env\n",
    "\n",
    "    if os.path.isfile(\"convrnn-s.weights\"):  # turn off for new model\n",
    "        model.load_state_dict(torch.load(\"convrnn-s.weights\", map_location='cpu'))\n",
    "        gmodel.load_state_dict(torch.load(\"convrnn-s.weights\", map_location='cpu'))\n",
    "        print(\"loaded checkpoint\")\n",
    "\n",
    "    if os.path.exists(\"training.txt\"):\n",
    "        os.remove(\"training.txt\")\n",
    "\n",
    "    rr = 0\n",
    "    ii = 0\n",
    "    for i in range(1001):\n",
    "        full_rollouts = [do_rollout(env, leif) for _ in range(ROLLOUTS_PER_BATCH)]\n",
    "        states, hns, cns, actions, rewards, gae = unroll_rollouts(gmodel, full_rollouts)\n",
    "        gmodel.gamma = 0.5 + 1 / 2. / (1 + math.exp(-0.0003 * (i - 20000)))  # adaptive gamma\n",
    "        l, pl, vl = gmodel_train(gmodel, states, hns, cns, actions, rewards, gae)\n",
    "        rr = rr * 0.99 + (np.mean(rewards) / len(actions)) / ROLLOUTS_PER_BATCH * 0.01\n",
    "        ii += len(actions)\n",
    "        print(i, \"\\t\", round(gmodel.gamma, 3), round(rr*1000, 3), \"\\twins:\", \"---\", Counter(actions),\n",
    "              round(sum(rewards), 3), round(l, 3), round(pl, 3), round(vl, 3))\n",
    "        with open(\"training.txt\", \"a\") as f:\n",
    "            print(rr, \"\\t\", round(gmodel.gamma, 4), \"\\t\", round(vl, 3), \"\\t\", round(pl, 3), \"\\t\", round(l, 3), file=f)\n",
    "        model.load_state_dict(gmodel.state_dict())\n",
    "        if i >= 10 and i % 30 == 0:\n",
    "            torch.save(gmodel.state_dict(), \"convrnn-s.weights\")\n",
    "            print(\"saved weights\")\n",
    "\n",
    "\n",
    "def run(world):\n",
    "    done, ded, state, _ = False, False, world.env.reset(), world.leif.clear()\n",
    "\n",
    "    while not done:\n",
    "        action = world.env.act(state)\n",
    "        state, reward, done, info = world.env.step(action)\n",
    "        print(world.leif.board_cent)\n",
    "        print(world.leif.bbs_cent)\n",
    "        print(world.leif.bl_cent)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    world.env.close()\n",
    "    return None\n",
    "\n",
    "\n",
    "def evaluate(world):\n",
    "    env = world.env\n",
    "    model = world.model\n",
    "    leif = world.leif\n",
    "    leif.debug = True\n",
    "    leif.stochastic = True\n",
    "\n",
    "    do_print = True\n",
    "    reward = 0\n",
    "\n",
    "    while True:\n",
    "        model.load_state_dict(torch.load(\"convrnn-s.weights\", map_location='cpu'))\n",
    "\n",
    "        done, state, _ = False, env.reset(), leif.clear()\n",
    "        t = 0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            if do_print:\n",
    "                time.sleep(0.1)\n",
    "                # os.system('clear')\n",
    "                print(state[0]['board'])\n",
    "                print(\"\\n\\n\")\n",
    "                print(\"Probs: \\t\", leif.probs[-1] if len(leif.probs) > 0 else [])\n",
    "                print(\"Val: \\t\", leif.values[-1] if len(leif.values) > 0 else None)\n",
    "                print(\"\\nReward: \", reward, \"Time\", t)\n",
    "\n",
    "            action = env.act(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            t += 1\n",
    "\n",
    "\n",
    "#evaluate(World())\n",
    "train(World())\n",
    "# alpha, fließender durchschnitsswert für rewards | winrate | 5 bombe 0 stehen | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "cvErs9qIYNMD",
    "outputId": "6ef40d27-685c-42e0-c88a-97fa8833dc62"
   },
   "outputs": [],
   "source": [
    "evaluate(World())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "poZ7rSNpFl9w",
    "outputId": "db8952da-7d6d-463a-c3d3-54e78f553011"
   },
   "outputs": [],
   "source": [
    "!git add .\n",
    "!git commit -m \"newest changes jg\"\n",
    "!git push"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "main.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import *\n",
    "import colorama\n",
    "from pommerman import agents\n",
    "from collections import Counter\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import numpy.matlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ROLLOUTS_PER_BATCH = 1\n",
    "batch = []\n",
    "\n",
    "\n",
    "class World:\n",
    "    def __init__(self, init_gmodel=True):\n",
    "        if init_gmodel:\n",
    "            self.gmodel = A2CNet(gpu=True)  # Global model\n",
    "\n",
    "        self.model = A2CNet(gpu=False)  # Agent (local) model\n",
    "        self.leif = Leif(self.model)\n",
    "        self.stoner = Stoner()\n",
    "\n",
    "        self.agent_list = [\n",
    "            self.leif,\n",
    "            # self.stoner\n",
    "            agents.SimpleAgent(),\n",
    "            agents.SimpleAgent(),\n",
    "            agents.SimpleAgent()\n",
    "        ]\n",
    "        self.env = normal_env(self.agent_list)  # naked_env\n",
    "        fmt = {\n",
    "            'int': self.color_sign,\n",
    "            'float': self.color_sign\n",
    "        }\n",
    "        np.set_printoptions(formatter=fmt, linewidth=300)\n",
    "        pass\n",
    "\n",
    "    def color_sign(self, x):\n",
    "        if x == 0:\n",
    "            c = colorama.Fore.LIGHTBLACK_EX\n",
    "        elif x == 1:\n",
    "            c = colorama.Fore.BLACK\n",
    "        elif x == 2:\n",
    "            c = colorama.Fore.BLUE\n",
    "        elif x == 3:\n",
    "            c = colorama.Fore.RED\n",
    "        elif x == 4:\n",
    "            c = colorama.Fore.RED\n",
    "        elif x == 10:\n",
    "            c = colorama.Fore.YELLOW\n",
    "        else:\n",
    "            c = colorama.Fore.WHITE\n",
    "        x = '{0: <2}'.format(x)\n",
    "        return f'{c}{x}{colorama.Fore.RESET}'\n",
    "\n",
    "\n",
    "def do_rollout(env, leif, do_print=False):\n",
    "    done, state = False, env.reset()\n",
    "    rewards, dones = [], []\n",
    "    states, actions, hidden, probs, values = leif.clear()\n",
    "    old_state = None\n",
    "    last_action = 0\n",
    "\n",
    "    while not done and 10 in state[0]['alive']:\n",
    "        if do_print:\n",
    "            time.sleep(0.1)\n",
    "            os.system('clear')\n",
    "            print(state[0]['board'])\n",
    "\n",
    "        action = env.act(state)\n",
    "        state, start_rewards, done, info = env.step(action)\n",
    "        action = action[0]\n",
    "        if old_state is None:\n",
    "            old_state = state\n",
    "        reward = get_reward(state, old_state, 0, action, last_action)\n",
    "        # print(str(state[0]['position']) + str(old_state[0]['position']) + str(reward))\n",
    "        old_state = state\n",
    "        last_action = action\n",
    "        rewards.append(reward)\n",
    "        dones.append(done)\n",
    "\n",
    "    hidden = hidden[:-1].copy()\n",
    "    hns, cns = [], []\n",
    "    for hns_cns_tuple in hidden:\n",
    "        hns.append(hns_cns_tuple[0])\n",
    "        cns.append(hns_cns_tuple[1])\n",
    "\n",
    "    rewards = rewards[:len(values)]\n",
    "\n",
    "    return (states.copy(),\n",
    "            actions.copy(),\n",
    "            rewards, dones,\n",
    "            (hns, cns),\n",
    "            probs.copy(),\n",
    "            values.copy())\n",
    "\n",
    "\n",
    "def get_reward(state, old_state, agent_nr, action, last_action):\n",
    "    # developer note: on the board:\n",
    "    # 0: nothing, 1: unbreakable wall, 2: wall, 3: bomb, 4: flames, 6,7,8: pick-ups:  11,12 and 13: enemies\n",
    "    reward = 0\n",
    "    # penalty for dying\n",
    "    if 10 not in state[0]['alive']:\n",
    "        reward -= 1\n",
    "\n",
    "    # reward stage 0:\n",
    "    # teach the agent to move and not make invalid actions (move into walls, place bombs when you have no ammo)\n",
    "    ammo = old_state[agent_nr]['ammo']\n",
    "    if action != 5:\n",
    "        if state[agent_nr]['position'] == old_state[agent_nr]['position']:\n",
    "            reward -= 0.03\n",
    "    elif ammo == 0:\n",
    "        reward -= 0.03\n",
    "\n",
    "    # reward stage 1: teach agent to bomb walls (and enemies)\n",
    "    # compute adjacent squares\n",
    "    position = state[agent_nr]['position']\n",
    "    adj = [(i, j) for i in (-1, 0, 1) for j in (-1, 0, 1) if not ((i == j) or i + j == 0)]\n",
    "    adjacent = numpy.matlib.repmat(position, 4, 1)\n",
    "    adjacent = adjacent - np.asarray(adj)\n",
    "    # limit adjacent squares to only include inside board\n",
    "    adjacent = np.clip(adjacent, 0, 10)\n",
    "    if action == 5 and ammo > 0:\n",
    "        board = state[agent_nr]['board']\n",
    "        for xy in adjacent:\n",
    "            square_val = board[xy[0]][xy[1]]\n",
    "            if square_val == 2:\n",
    "                reward += 0.2\n",
    "            elif square_val == 11 or square_val == 12 or square_val == 13:\n",
    "                reward += 0.5\n",
    "\n",
    "    # reward stage2: teach agent to not stand on or beside bombs\n",
    "    # reward /= 4\n",
    "    bomb_life = state[agent_nr]['bomb_life']\n",
    "    # if we stand on a bomb or next to bomb\n",
    "    just_placed_bomb = np.logical_xor(last_action == 5, action == 5)\n",
    "    if bomb_life[position] > 0 and not just_placed_bomb:\n",
    "        reward -= 0.1 * (9-bomb_life[position])\n",
    "    for xy in adjacent:\n",
    "        if bomb_life[xy[0]][xy[1]] > 0:\n",
    "            reward -= 0.05 * (9-bomb_life[xy[0]][xy[1]])\n",
    "\n",
    "    # reward agent for picking up power-ups\n",
    "    blast_strength = state[agent_nr]['blast_strength']\n",
    "    old_blast_strength = old_state[agent_nr]['blast_strength']\n",
    "    can_kick = int(state[agent_nr]['can_kick'])\n",
    "    old_can_kick = int(old_state[agent_nr]['can_kick'])\n",
    "    reward += (can_kick-old_can_kick)*0.02\n",
    "    # reward += (max_ammo-old_max_ammo)*0.02 #TODO, see arguments\n",
    "    reward += (blast_strength-old_blast_strength)*0.02\n",
    "    return reward\n",
    "\n",
    "\n",
    "def gmodel_train(gmodel, states, hns, cns, actions, rewards, gae):\n",
    "    states, hns, cns = torch.stack(states), torch.stack(hns, dim=0), torch.stack(cns, dim=0)\n",
    "    gmodel.train()\n",
    "    probs, values, _, _ = gmodel(states.to(gmodel.device), hns.to(gmodel.device), cns.to(gmodel.device), debug=False)\n",
    "\n",
    "    prob = F.softmax(probs, dim=-1)\n",
    "    log_prob = F.log_softmax(probs, dim=-1)\n",
    "    entropy = -(log_prob * prob).sum(1)\n",
    "\n",
    "    log_probs = log_prob[range(0, len(actions)), actions]\n",
    "    advantages = torch.tensor(rewards).to(gmodel.device) - values.squeeze(1)\n",
    "    value_loss = advantages.pow(2) * 0.5\n",
    "    policy_loss = -log_probs * torch.tensor(gae).to(gmodel.device) - gmodel.entropy_coef * entropy\n",
    "\n",
    "    gmodel.optimizer.zero_grad()\n",
    "    pl = policy_loss.sum()\n",
    "    vl = value_loss.sum()\n",
    "    loss = pl + vl\n",
    "    loss.backward()\n",
    "    gmodel.optimizer.step()\n",
    "\n",
    "    return loss.item(), pl.item(), vl.item()\n",
    "\n",
    "\n",
    "def unroll_rollouts(gmodel, list_of_full_rollouts):\n",
    "    gamma = gmodel.gamma\n",
    "    tau = 1\n",
    "\n",
    "    states, actions, rewards, hns, cns, gae = [], [], [], [], [], []\n",
    "    for (s, a, r, d, h, p, v) in list_of_full_rollouts:\n",
    "        states.extend(torch.tensor(s))\n",
    "        actions.extend(a)\n",
    "        rewards.extend(gmodel.discount_rewards(r))\n",
    "\n",
    "        hns.extend([torch.tensor(hh) for hh in h[0]])\n",
    "        cns.extend([torch.tensor(hh) for hh in h[1]])\n",
    "\n",
    "        # Calculate GAE\n",
    "        last_i, _gae, __gae = len(r) - 1, [], 0\n",
    "        for i in reversed(range(len(r))):\n",
    "            next_val = v[i + 1] if i != last_i else 0\n",
    "            delta_t = r[i] + gamma * next_val - v[i]\n",
    "            __gae = __gae * gamma * tau + delta_t\n",
    "            _gae.insert(0, __gae)\n",
    "\n",
    "        gae.extend(_gae)\n",
    "\n",
    "    return states, hns, cns, actions, rewards, gae\n",
    "\n",
    "\n",
    "def train(world):\n",
    "    model, gmodel = world.model, world.gmodel\n",
    "    leif, env = world.leif, world.env\n",
    "\n",
    "    if os.path.isfile(\"convrnn-s.weights\"):  # turn off for new model\n",
    "        model.load_state_dict(torch.load(\"convrnn-s.weights\", map_location='cpu'))\n",
    "        gmodel.load_state_dict(torch.load(\"convrnn-s.weights\", map_location='cpu'))\n",
    "        print(\"loaded checkpoint\")\n",
    "\n",
    "    if os.path.exists(\"training.txt\"):\n",
    "        os.remove(\"training.txt\")\n",
    "\n",
    "    rr = 0\n",
    "    ii = 0\n",
    "    for i in range(30001):\n",
    "        full_rollouts = [do_rollout(env, leif) for _ in range(ROLLOUTS_PER_BATCH)]\n",
    "        states, hns, cns, actions, rewards, gae = unroll_rollouts(gmodel, full_rollouts)\n",
    "        gmodel.gamma = 0.5 + 1 / 2. / (1 + math.exp(-0.0003 * (i - 20000)))  # adaptive gamma\n",
    "        l, pl, vl = gmodel_train(gmodel, states, hns, cns, actions, rewards, gae)\n",
    "        rr = rr * 0.99 + (np.mean(rewards) / len(actions)) / ROLLOUTS_PER_BATCH * 0.01\n",
    "        ii += len(actions)\n",
    "        print(i, \"\\t\", round(gmodel.gamma, 3), round(rr*1000, 3), \"\\twins:\", \"---\", Counter(actions),\n",
    "              round(sum(rewards), 3), round(l, 3), round(pl, 3), round(vl, 3))\n",
    "        with open(\"training.txt\", \"a\") as f:\n",
    "            print(rr, \"\\t\", round(gmodel.gamma, 4), \"\\t\", round(vl, 3), \"\\t\", round(pl, 3), \"\\t\", round(l, 3), file=f)\n",
    "        model.load_state_dict(gmodel.state_dict())\n",
    "        if i >= 10 and i % 30 == 0:\n",
    "            torch.save(gmodel.state_dict(), \"convrnn-s.weights\")\n",
    "            print(\"saved weights\")\n",
    "\n",
    "\n",
    "def run(world):\n",
    "    done, ded, state, _ = False, False, world.env.reset(), world.leif.clear()\n",
    "\n",
    "    while not done:\n",
    "        action = world.env.act(state)\n",
    "        state, reward, done, info = world.env.step(action)\n",
    "        print(world.leif.board_cent)\n",
    "        print(world.leif.bbs_cent)\n",
    "        print(world.leif.bl_cent)\n",
    "        time.sleep(0.2)\n",
    "\n",
    "    world.env.close()\n",
    "    return None\n",
    "\n",
    "\n",
    "def evaluate(world):\n",
    "    env = world.env\n",
    "    model = world.model\n",
    "    leif = world.leif\n",
    "    leif.debug = True\n",
    "    leif.stochastic = True\n",
    "\n",
    "    do_print = True\n",
    "    reward = 0\n",
    "\n",
    "    while True:\n",
    "        model.load_state_dict(torch.load(\"convrnn-s.weights\", map_location='cpu'))\n",
    "\n",
    "        done, state, _ = False, env.reset(), leif.clear()\n",
    "        t = 0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            if do_print:\n",
    "                time.sleep(0.1)\n",
    "                # os.system('clear')\n",
    "                print(state[0]['board'])\n",
    "                print(\"\\n\\n\")\n",
    "                print(\"Probs: \\t\", leif.probs[-1] if len(leif.probs) > 0 else [])\n",
    "                print(\"Val: \\t\", leif.values[-1] if len(leif.values) > 0 else None)\n",
    "                print(\"\\nReward: \", reward, \"Time\", t)\n",
    "\n",
    "            action = env.act(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            t += 1\n",
    "\n",
    "\n",
    "evaluate(World())\n",
    "# train(World())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

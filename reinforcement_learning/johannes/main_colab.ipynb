{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"jonnygaese@gmail.com\"\n",
        "!git config --global user.name \"JohnnyG502\""
      ],
      "metadata": {
        "id": "_UkmnhYjFXsx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XoOotFoMT0S9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://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@github.com/JohnnyG502/pommermanAI.git"
      ],
      "metadata": {
        "id": "hg0Y93n1Faxv",
        "outputId": "a2400a02-4a72-4694-cf3a-92ec3e5705eb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pommermanAI'...\n",
            "remote: Enumerating objects: 314, done.\u001b[K\n",
            "remote: Counting objects: 100% (314/314), done.\u001b[K\n",
            "remote: Compressing objects: 100% (247/247), done.\u001b[K\n",
            "remote: Total 314 (delta 85), reused 239 (delta 47), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (314/314), 4.29 MiB | 6.75 MiB/s, done.\n",
            "Resolving deltas: 100% (85/85), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd pommermanAI/reinforcement_learning/johannes"
      ],
      "metadata": {
        "id": "xy1CHIukFfgY",
        "outputId": "6222ce04-9aef-4bcb-d78c-07bbbaf85be2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pommermanAI/pommermanAI/reinforcement_learning/johannes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull origin main"
      ],
      "metadata": {
        "id": "FnuvZ2IpFhvj",
        "outputId": "6b0c91cd-8a11-452e-9739-01af4078e98a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Counting objects:   6% (1/16)\u001b[K\rremote: Counting objects:  12% (2/16)\u001b[K\rremote: Counting objects:  18% (3/16)\u001b[K\rremote: Counting objects:  25% (4/16)\u001b[K\rremote: Counting objects:  31% (5/16)\u001b[K\rremote: Counting objects:  37% (6/16)\u001b[K\rremote: Counting objects:  43% (7/16)\u001b[K\rremote: Counting objects:  50% (8/16)\u001b[K\rremote: Counting objects:  56% (9/16)\u001b[K\rremote: Counting objects:  62% (10/16)\u001b[K\rremote: Counting objects:  68% (11/16)\u001b[K\rremote: Counting objects:  75% (12/16)\u001b[K\rremote: Counting objects:  81% (13/16)\u001b[K\rremote: Counting objects:  87% (14/16)\u001b[K\rremote: Counting objects:  93% (15/16)\u001b[K\rremote: Counting objects: 100% (16/16)\u001b[K\rremote: Counting objects: 100% (16/16), done.\u001b[K\n",
            "remote: Compressing objects:  16% (1/6)\u001b[K\rremote: Compressing objects:  33% (2/6)\u001b[K\rremote: Compressing objects:  50% (3/6)\u001b[K\rremote: Compressing objects:  66% (4/6)\u001b[K\rremote: Compressing objects:  83% (5/6)\u001b[K\rremote: Compressing objects: 100% (6/6)\u001b[K\rremote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 10 (delta 4), reused 8 (delta 4), pack-reused 0\u001b[K\n",
            "Unpacking objects:  10% (1/10)   \rUnpacking objects:  20% (2/10)   \rUnpacking objects:  30% (3/10)   \rUnpacking objects:  40% (4/10)   \rUnpacking objects:  50% (5/10)   \rUnpacking objects:  60% (6/10)   \rUnpacking objects:  70% (7/10)   \rUnpacking objects:  80% (8/10)   \rUnpacking objects:  90% (9/10)   \rUnpacking objects: 100% (10/10)   \rUnpacking objects: 100% (10/10), done.\n",
            "From https://github.com/JohnnyG502/pommermanAI\n",
            " * branch            main       -> FETCH_HEAD\n",
            "   b416656..435f6de  main       -> origin/main\n",
            "Updating b416656..435f6de\n",
            "Fast-forward\n",
            " reinforcement_learning/johannes/main.py  | 282 \u001b[32m++++++++++++++++++++++++++++++\u001b[m\n",
            " reinforcement_learning/johannes/model.py | 283 \u001b[32m+++++++++++++++++++++++++++++++\u001b[m\n",
            " 2 files changed, 565 insertions(+)\n",
            " create mode 100644 reinforcement_learning/johannes/main.py\n",
            " create mode 100644 reinforcement_learning/johannes/model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "H1EWPrKaF8-H",
        "outputId": "d8c57cc5-f95d-405e-b849-4b4b6e495f4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/reinforcement_learning/johannes/model_action_prun_colab')\n"
      ],
      "metadata": {
        "id": "14bTPofhHzgr"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n"
      ],
      "metadata": {
        "id": "DX5lG6HYNRzp"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files.os"
      ],
      "metadata": {
        "id": "XX04h1stNTTl",
        "outputId": "b8ef265b-d80e-4d44-f63c-3466caba4816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-c7c3359668a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'google.colab.files' has no attribute 'os'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "OeIGU3hgFOrL",
        "outputId": "20993bfb-1474-4dba-91b3-fb6af150b409",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-756b4b0761af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcolorama\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpommerman\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/pommermanAI/pommermanAI/reinforcement_learning/johannes/model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpommerman\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpommerman\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0magents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pommerman'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import model \n",
        "import colorama\n",
        "from pommerman import agents\n",
        "from collections import Counter\n",
        "import time\n",
        "import math\n",
        "import os\n",
        "import numpy.matlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neuer Abschnitt"
      ],
      "metadata": {
        "id": "tSJfyfjoMpnT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_blBAK9TFOrR"
      },
      "outputs": [],
      "source": [
        "\n",
        "ROLLOUTS_PER_BATCH = 1\n",
        "batch = []\n",
        "\n",
        "\n",
        "class World:\n",
        "    def __init__(self, init_gmodel=True):\n",
        "        if init_gmodel:\n",
        "            self.gmodel = A2CNet(gpu=True)  # Global model\n",
        "\n",
        "        self.model = A2CNet(gpu=False)  # Agent (local) model\n",
        "        self.leif = Leif(self.model)\n",
        "        self.stoner = Stoner()\n",
        "\n",
        "        self.agent_list = [\n",
        "            self.leif,\n",
        "            # self.stoner\n",
        "            agents.SimpleAgent(),\n",
        "            agents.SimpleAgent(),\n",
        "            agents.SimpleAgent()\n",
        "        ]\n",
        "        self.env = normal_env(self.agent_list)  # naked_env\n",
        "        fmt = {\n",
        "            'int': self.color_sign,\n",
        "            'float': self.color_sign\n",
        "        }\n",
        "        np.set_printoptions(formatter=fmt, linewidth=300)\n",
        "        pass\n",
        "\n",
        "    def color_sign(self, x):\n",
        "        if x == 0:\n",
        "            c = colorama.Fore.LIGHTBLACK_EX\n",
        "        elif x == 1:\n",
        "            c = colorama.Fore.BLACK\n",
        "        elif x == 2:\n",
        "            c = colorama.Fore.BLUE\n",
        "        elif x == 3:\n",
        "            c = colorama.Fore.RED\n",
        "        elif x == 4:\n",
        "            c = colorama.Fore.RED\n",
        "        elif x == 10:\n",
        "            c = colorama.Fore.YELLOW\n",
        "        else:\n",
        "            c = colorama.Fore.WHITE\n",
        "        x = '{0: <2}'.format(x)\n",
        "        return f'{c}{x}{colorama.Fore.RESET}'\n",
        "\n",
        "\n",
        "def do_rollout(env, leif, do_print=False):\n",
        "    done, state = False, env.reset()\n",
        "    rewards, dones = [], []\n",
        "    states, actions, hidden, probs, values = leif.clear()\n",
        "    old_state = None\n",
        "    last_action = 0\n",
        "\n",
        "    while not done and 10 in state[0]['alive']:\n",
        "        if do_print:\n",
        "            time.sleep(0.1)\n",
        "            os.system('clear')\n",
        "            print(state[0]['board'])\n",
        "\n",
        "        action = env.act(state)\n",
        "        state, start_rewards, done, info = env.step(action)\n",
        "        action = action[0]\n",
        "        if old_state is None:\n",
        "            old_state = state\n",
        "        reward = get_reward(state, old_state, 0, action, last_action)\n",
        "        # print(str(state[0]['position']) + str(old_state[0]['position']) + str(reward))\n",
        "        old_state = state\n",
        "        last_action = action\n",
        "        rewards.append(reward)\n",
        "        dones.append(done)\n",
        "\n",
        "    hidden = hidden[:-1].copy()\n",
        "    hns, cns = [], []\n",
        "    for hns_cns_tuple in hidden:\n",
        "        hns.append(hns_cns_tuple[0])\n",
        "        cns.append(hns_cns_tuple[1])\n",
        "\n",
        "    rewards = rewards[:len(values)]\n",
        "\n",
        "    return (states.copy(),\n",
        "            actions.copy(),\n",
        "            rewards, dones,\n",
        "            (hns, cns),\n",
        "            probs.copy(),\n",
        "            values.copy())\n",
        "\n",
        "\n",
        "def get_reward(state, old_state, agent_nr, action, last_action):\n",
        "    # developer note: on the board:\n",
        "    # 0: nothing, 1: unbreakable wall, 2: wall, 3: bomb, 4: flames, 6,7,8: pick-ups:  11,12 and 13: enemies\n",
        "    reward = 0\n",
        "    # penalty for dying\n",
        "    if 10 not in state[0]['alive']:\n",
        "        reward -= 1\n",
        "\n",
        "    # reward stage 0:\n",
        "    # teach the agent to move and not make invalid actions (move into walls, place bombs when you have no ammo)\n",
        "    ammo = old_state[agent_nr]['ammo']\n",
        "    if action != 5:\n",
        "        if state[agent_nr]['position'] == old_state[agent_nr]['position']:\n",
        "            reward -= 0.03\n",
        "    elif ammo == 0:\n",
        "        reward -= 0.03\n",
        "\n",
        "    # reward stage 1: teach agent to bomb walls (and enemies)\n",
        "    # compute adjacent squares\n",
        "    position = state[agent_nr]['position']\n",
        "    adj = [(i, j) for i in (-1, 0, 1) for j in (-1, 0, 1) if not ((i == j) or i + j == 0)]\n",
        "    adjacent = numpy.matlib.repmat(position, 4, 1)\n",
        "    adjacent = adjacent - np.asarray(adj)\n",
        "    # limit adjacent squares to only include inside board\n",
        "    adjacent = np.clip(adjacent, 0, 10)\n",
        "    if action == 5 and ammo > 0:\n",
        "        board = state[agent_nr]['board']\n",
        "        for xy in adjacent:\n",
        "            square_val = board[xy[0]][xy[1]]\n",
        "            if square_val == 2:\n",
        "                reward += 0.2\n",
        "            elif square_val == 11 or square_val == 12 or square_val == 13:\n",
        "                reward += 0.5\n",
        "\n",
        "    # reward stage2: teach agent to not stand on or beside bombs\n",
        "    # reward /= 4\n",
        "    bomb_life = state[agent_nr]['bomb_life']\n",
        "    # if we stand on a bomb or next to bomb\n",
        "    just_placed_bomb = np.logical_xor(last_action == 5, action == 5)\n",
        "    if bomb_life[position] > 0 and not just_placed_bomb:\n",
        "        reward -= 0.1 * (9-bomb_life[position])\n",
        "    for xy in adjacent:\n",
        "        if bomb_life[xy[0]][xy[1]] > 0:\n",
        "            reward -= 0.05 * (9-bomb_life[xy[0]][xy[1]])\n",
        "\n",
        "    # reward agent for picking up power-ups\n",
        "    blast_strength = state[agent_nr]['blast_strength']\n",
        "    old_blast_strength = old_state[agent_nr]['blast_strength']\n",
        "    can_kick = int(state[agent_nr]['can_kick'])\n",
        "    old_can_kick = int(old_state[agent_nr]['can_kick'])\n",
        "    reward += (can_kick-old_can_kick)*0.02\n",
        "    # reward += (max_ammo-old_max_ammo)*0.02 #TODO, see arguments\n",
        "    reward += (blast_strength-old_blast_strength)*0.02\n",
        "    return reward\n",
        "\n",
        "\n",
        "def gmodel_train(gmodel, states, hns, cns, actions, rewards, gae):\n",
        "    states, hns, cns = torch.stack(states), torch.stack(hns, dim=0), torch.stack(cns, dim=0)\n",
        "    gmodel.train()\n",
        "    probs, values, _, _ = gmodel(states.to(gmodel.device), hns.to(gmodel.device), cns.to(gmodel.device), debug=False)\n",
        "\n",
        "    prob = F.softmax(probs, dim=-1)\n",
        "    log_prob = F.log_softmax(probs, dim=-1)\n",
        "    entropy = -(log_prob * prob).sum(1)\n",
        "\n",
        "    log_probs = log_prob[range(0, len(actions)), actions]\n",
        "    advantages = torch.tensor(rewards).to(gmodel.device) - values.squeeze(1)\n",
        "    value_loss = advantages.pow(2) * 0.5\n",
        "    policy_loss = -log_probs * torch.tensor(gae).to(gmodel.device) - gmodel.entropy_coef * entropy\n",
        "\n",
        "    gmodel.optimizer.zero_grad()\n",
        "    pl = policy_loss.sum()\n",
        "    vl = value_loss.sum()\n",
        "    loss = pl + vl\n",
        "    loss.backward()\n",
        "    gmodel.optimizer.step()\n",
        "\n",
        "    return loss.item(), pl.item(), vl.item()\n",
        "\n",
        "\n",
        "def unroll_rollouts(gmodel, list_of_full_rollouts):\n",
        "    gamma = gmodel.gamma\n",
        "    tau = 1\n",
        "\n",
        "    states, actions, rewards, hns, cns, gae = [], [], [], [], [], []\n",
        "    for (s, a, r, d, h, p, v) in list_of_full_rollouts:\n",
        "        states.extend(torch.tensor(s))\n",
        "        actions.extend(a)\n",
        "        rewards.extend(gmodel.discount_rewards(r))\n",
        "\n",
        "        hns.extend([torch.tensor(hh) for hh in h[0]])\n",
        "        cns.extend([torch.tensor(hh) for hh in h[1]])\n",
        "\n",
        "        # Calculate GAE\n",
        "        last_i, _gae, __gae = len(r) - 1, [], 0\n",
        "        for i in reversed(range(len(r))):\n",
        "            next_val = v[i + 1] if i != last_i else 0\n",
        "            delta_t = r[i] + gamma * next_val - v[i]\n",
        "            __gae = __gae * gamma * tau + delta_t\n",
        "            _gae.insert(0, __gae)\n",
        "\n",
        "        gae.extend(_gae)\n",
        "\n",
        "    return states, hns, cns, actions, rewards, gae\n",
        "\n",
        "\n",
        "def train(world):\n",
        "    model, gmodel = world.model, world.gmodel\n",
        "    leif, env = world.leif, world.env\n",
        "\n",
        "    if os.path.isfile(\"convrnn-s.weights\"):  # turn off for new model\n",
        "        model.load_state_dict(torch.load(\"convrnn-s.weights\", map_location='cpu'))\n",
        "        gmodel.load_state_dict(torch.load(\"convrnn-s.weights\", map_location='cpu'))\n",
        "        print(\"loaded checkpoint\")\n",
        "\n",
        "    if os.path.exists(\"training.txt\"):\n",
        "        os.remove(\"training.txt\")\n",
        "\n",
        "    rr = 0\n",
        "    ii = 0\n",
        "    for i in range(30001):\n",
        "        full_rollouts = [do_rollout(env, leif) for _ in range(ROLLOUTS_PER_BATCH)]\n",
        "        states, hns, cns, actions, rewards, gae = unroll_rollouts(gmodel, full_rollouts)\n",
        "        gmodel.gamma = 0.5 + 1 / 2. / (1 + math.exp(-0.0003 * (i - 20000)))  # adaptive gamma\n",
        "        l, pl, vl = gmodel_train(gmodel, states, hns, cns, actions, rewards, gae)\n",
        "        rr = rr * 0.99 + (np.mean(rewards) / len(actions)) / ROLLOUTS_PER_BATCH * 0.01\n",
        "        ii += len(actions)\n",
        "        print(i, \"\\t\", round(gmodel.gamma, 3), round(rr*1000, 3), \"\\twins:\", \"---\", Counter(actions),\n",
        "              round(sum(rewards), 3), round(l, 3), round(pl, 3), round(vl, 3))\n",
        "        with open(\"training.txt\", \"a\") as f:\n",
        "            print(rr, \"\\t\", round(gmodel.gamma, 4), \"\\t\", round(vl, 3), \"\\t\", round(pl, 3), \"\\t\", round(l, 3), file=f)\n",
        "        model.load_state_dict(gmodel.state_dict())\n",
        "        if i >= 10 and i % 30 == 0:\n",
        "            torch.save(gmodel.state_dict(), \"convrnn-s.weights\")\n",
        "            print(\"saved weights\")\n",
        "\n",
        "\n",
        "def run(world):\n",
        "    done, ded, state, _ = False, False, world.env.reset(), world.leif.clear()\n",
        "\n",
        "    while not done:\n",
        "        action = world.env.act(state)\n",
        "        state, reward, done, info = world.env.step(action)\n",
        "        print(world.leif.board_cent)\n",
        "        print(world.leif.bbs_cent)\n",
        "        print(world.leif.bl_cent)\n",
        "        time.sleep(0.2)\n",
        "\n",
        "    world.env.close()\n",
        "    return None\n",
        "\n",
        "\n",
        "def evaluate(world):\n",
        "    env = world.env\n",
        "    model = world.model\n",
        "    leif = world.leif\n",
        "    leif.debug = True\n",
        "    leif.stochastic = True\n",
        "\n",
        "    do_print = True\n",
        "    reward = 0\n",
        "\n",
        "    while True:\n",
        "        model.load_state_dict(torch.load(\"convrnn-s.weights\", map_location='cpu'))\n",
        "\n",
        "        done, state, _ = False, env.reset(), leif.clear()\n",
        "        t = 0\n",
        "        while not done:\n",
        "            env.render()\n",
        "            if do_print:\n",
        "                time.sleep(0.1)\n",
        "                # os.system('clear')\n",
        "                print(state[0]['board'])\n",
        "                print(\"\\n\\n\")\n",
        "                print(\"Probs: \\t\", leif.probs[-1] if len(leif.probs) > 0 else [])\n",
        "                print(\"Val: \\t\", leif.values[-1] if len(leif.values) > 0 else None)\n",
        "                print(\"\\nReward: \", reward, \"Time\", t)\n",
        "\n",
        "            action = env.act(state)\n",
        "            state, reward, done, info = env.step(action)\n",
        "            t += 1\n",
        "\n",
        "\n",
        "evaluate(World())\n",
        "# train(World())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git add .\n",
        "!git commit -m \"newest changes\"\n",
        "!git push"
      ],
      "metadata": {
        "id": "poZ7rSNpFl9w",
        "outputId": "d10a8aae-b589-49f5-e87a-b30d8964d6b4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "Everything up-to-date\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "main.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}